{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c8a073-bd3d-4acf-82a9-0eaf3233cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  8 17:10:34 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.00    Driver Version: 470.82.00    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   56C    P8    21W /  N/A |    701MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f649107-5002-4b2d-8c80-d202778a970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae9da1-f27d-48bc-8b9b-90426504cfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f189a1-856c-4928-9005-93b8874d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- daniel testing ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab31ac71-4cf9-4629-a842-04f73e713db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# data = []\n",
    "# with open('manifest.json') as f:\n",
    "#     for line in f:\n",
    "#         data.append(json.loads(line))\n",
    "        \n",
    "# with open('PRIV.txt', 'w+') as f:\n",
    "#     for i, j in enumerate(data):\n",
    "#         f.write(f\"PRIV_{i:03} {data[i]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99d3de7-62d6-4dfd-9ad6-6a7384399c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- daniel testing ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169194de-cd64-4f57-87d3-5a5151d8f87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068ab21-c48c-4546-86af-a19e44fb1ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa74b2-55dd-49ca-8462-be3120b9c589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484acc-e61c-4a78-a628-11ec4e832ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3775810-a515-4a53-81fb-c3fd9f7ae98e",
   "metadata": {},
   "source": [
    "# Code for finetuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb365a-ec07-4521-9642-e54eb5d581bb",
   "metadata": {},
   "source": [
    "## Convert the dataset imported from the pkl files into a DatasetDict object for training on the transformers later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0a581c-e554-4d56-9ca8-7f688e8b2082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 1818\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# check the format of the DatasetDict\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f2305-501f-4b2e-b3cd-ad056bf14e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33f34440-5232-4e5c-84b1-cd6fcac6c850",
   "metadata": {},
   "source": [
    "## Preparing the feature extractor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eadb05ea-4893-4e52-87c1-e80ce5a5924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all characters available in the train and dev datasets\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6aab06f-b48e-4a9c-90da-d9339810c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract characters from train dataset\n",
    "vocabs_train = extract_all_chars(df_train)\n",
    "\n",
    "# extract characters from dev dataset\n",
    "vocabs_dev = extract_all_chars(df_dev)\n",
    "\n",
    "# create a union of all distinct letters in the training and the dev datasets\n",
    "vocab_list = list(set(vocabs_train) | set(vocabs_dev))\n",
    "\n",
    "# convert resulting list into an enumerated dictionary\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "# replace space with a more visible character |\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "# add the [UNK], [PAD], bos and eos token\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "vocab_dict[\"<s>\"] = len(vocab_dict)\n",
    "vocab_dict[\"</s>\"] = len(vocab_dict)\n",
    "\n",
    "# make the useless vocabs as [UNK] in the end\n",
    "del vocab_dict[\"#\"]\n",
    "del vocab_dict[\"-\"]\n",
    "\n",
    "# renumber the dictionary values to fill up the blanks\n",
    "count = 0\n",
    "for key, value in vocab_dict.items():\n",
    "    vocab_dict[key] = count\n",
    "    count += 1\n",
    "    \n",
    "# vocabulary is completed, now save the vocabulary as a json file\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5b7c37-79fb-4d47-ae1f-8bbfa93b2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE TOKENIZER\n",
    "# use the json file to instantiate an object of the Wav2Vec2CTCTokenizer class\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\", bos_token='<s>', eos_token='</s>')\n",
    "\n",
    "# after the tokenizer object is created, the vocab.json file is not needed anymore, since the processor file will be created and the vocab.json will be there, hence can remove it\n",
    "os.remove('vocab.json')\n",
    "\n",
    "# PREPARING THE FEATURE EXTRACTOR\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "\n",
    "# wrap the feature extractor and tokenizer as a single Wav2VevProcessor class object\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fa6b1-0757-4d98-b225-b1a20f9d9f59",
   "metadata": {},
   "source": [
    "## Further preparation of the dataset after defining the processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63e14f-3c74-4261-ad91-29f543df992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80dc24c-4b5e-4924-b5ea-2145867b1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the format of the DatasetDict again with the required values needed for the training later\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f75680-6f0f-48fa-9c82-ed7d7fce0a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75548e34-0e28-4abf-a11e-ecc764d51078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13cf0f1d-b570-4284-b7b4-dd05b804eb0c",
   "metadata": {},
   "source": [
    "## Get the audio sample distribution (16k sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a09a7-6f42-452d-8c32-73215430822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list to get the list of audio length of all the training data\n",
    "audio_length_list = []\n",
    "for idx, item in tqdm(enumerate(dataset['train'])):\n",
    "    audio_length_list.append(dataset['train'][idx]['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc2c99-5284-4fc2-8195-c656017496db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution of the audio sample\n",
    "data_dist = pd.Series(audio_length_list)\n",
    "\n",
    "data_dist.plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Number of inputs')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55711d-a263-43ce-9ab6-0e0b6c2055b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution of the audio length (in 16k sample)\n",
    "data_dist = pd.Series([audio_length/16000 for audio_length in audio_length_list])\n",
    "\n",
    "data_dist.plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Length of Audio')\n",
    "plt.ylabel('Number of inputs')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abc9d1-dc1f-448c-a6b5-e4eb95815e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f756be25-978c-4237-9e92-b43b09f37bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186354b-7852-43ee-85c3-0f6964c9d8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7870d-62b6-4954-b8fb-ba9300fae166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aeff6b4-f7e4-47a5-bbde-c0b42b9535e2",
   "metadata": {},
   "source": [
    "## Set up Trainer class for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286b565-f512-4474-9ab6-9975407bf5db",
   "metadata": {},
   "source": [
    "In contrast to the common data collators, this data collator treats the input_values and labels differently and thus applies to separate padding functions on them (again making use of Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function. Analogous to the common data collators, the padding tokens in the labels with -100 so that those tokens are not taken into account when computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cadd8a1f-7a42-49ff-b1c0-037f8523bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76225a7d-cf69-47d0-9ea4-aae1db44e209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorCTCWithPadding(processor=Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='', vocab_size=32, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}), padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4170cf-8687-4a1d-a3c7-de8a6e8613fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8548ef2-2db7-479a-b7c8-4ca33274316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0757e2-f60e-402d-b8a4-fec215c06cf2",
   "metadata": {},
   "source": [
    "The model will return a sequence of logit vectors:\n",
    "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
    "\n",
    "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa228d-336a-44ea-9be3-0f70fdb25ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aff7dd-639b-45dd-b82a-e6854962e930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e60086af-4e19-4bb6-a2e8-31c1dda8dd5b",
   "metadata": {},
   "source": [
    "Now, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5034fa-eef4-4fe3-97fd-b346555cc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained wav2vec2 checkpoint - # load model from local instead of downloading from internet\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    #\"facebook/wav2vec2-base\",\n",
    "    './wav2vec2_base_model/',\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e3bef-f9da-499a-8ae5-4ee66698dcba",
   "metadata": {},
   "source": [
    "The first component of Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretrainind and as stated in the [paper](https://arxiv.org/abs/2006.11477) does not need to be fine-tuned anymore. \n",
    "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part.  \n",
    "\n",
    "In a final step, we define all parameters related to training. \n",
    "To give more explanation on some of the parameters:\n",
    "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
    "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Timit dataset and might be suboptimal for other speech datasets.\n",
    "\n",
    "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
    "\n",
    "During training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. It allows you to also play around with the demo widget even while your model is still training.\n",
    "\n",
    "**Note**: If one does not want to upload the model checkpoints to the hub, simply set `push_to_hub=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af625440-6d50-4e4b-8444-542a8fba107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./ckpt',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,#30\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=200,#500,\n",
    "  eval_steps=200,#500,\n",
    "  logging_steps=200,#500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "483b3488-d592-4617-9fe0-6908e7ab0788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# defining the training class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f33b1b71-5547-4ba1-be71-bd7f77762b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1796\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 41:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>29.554500</td>\n",
       "      <td>9.377405</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>14.998100</td>\n",
       "      <td>7.772687</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.890600</td>\n",
       "      <td>3.180052</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.294000</td>\n",
       "      <td>3.075372</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.529200</td>\n",
       "      <td>2.996717</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.280900</td>\n",
       "      <td>3.041261</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.028800</td>\n",
       "      <td>1.684989</td>\n",
       "      <td>0.895941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.045000</td>\n",
       "      <td>0.673530</td>\n",
       "      <td>0.505873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.497200</td>\n",
       "      <td>0.585380</td>\n",
       "      <td>0.377601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.124600</td>\n",
       "      <td>0.476169</td>\n",
       "      <td>0.308881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>0.444708</td>\n",
       "      <td>0.284154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-200\n",
      "Configuration saved in ./ckpt/checkpoint-200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-200/preprocessor_config.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-400\n",
      "Configuration saved in ./ckpt/checkpoint-400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-400/preprocessor_config.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-600\n",
      "Configuration saved in ./ckpt/checkpoint-600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-800\n",
      "Configuration saved in ./ckpt/checkpoint-800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1000\n",
      "Configuration saved in ./ckpt/checkpoint-1000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1200\n",
      "Configuration saved in ./ckpt/checkpoint-1200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1400\n",
      "Configuration saved in ./ckpt/checkpoint-1400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1600\n",
      "Configuration saved in ./ckpt/checkpoint-1600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1800\n",
      "Configuration saved in ./ckpt/checkpoint-1800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2000\n",
      "Configuration saved in ./ckpt/checkpoint-2000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2200\n",
      "Configuration saved in ./ckpt/checkpoint-2200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=6.1912327423095705, metrics={'train_runtime': 2496.3366, 'train_samples_per_second': 7.195, 'train_steps_per_second': 0.901, 'total_flos': 1.677600751135104e+18, 'train_loss': 6.1912327423095705, 'epoch': 10.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start the finetuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d1b96-1ac6-47db-a3b5-82a2e77d43ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b3739d1-a262-4f02-96ce-4de878cb138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_model\n",
      "Configuration saved in ./saved_model/config.json\n",
      "Model weights saved in ./saved_model/pytorch_model.bin\n",
      "Feature extractor saved in ./saved_model/preprocessor_config.json\n",
      "Feature extractor saved in ./processor/preprocessor_config.json\n",
      "tokenizer config file saved in ./processor/tokenizer_config.json\n",
      "Special tokens file saved in ./processor/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save model to local directory\n",
    "trainer.save_state()\n",
    "trainer.save_model('./saved_model')\n",
    "# save the processor\n",
    "processor.save_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf5964-6736-4027-9dbc-79d7d67074cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b22775-bdb6-465c-8e9d-957e2a90a322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd3311c-31b3-4f78-b44c-3399a3b79c60",
   "metadata": {},
   "source": [
    "## Code to resume training/finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa746f6-6cd1-473b-9019-18eb12c59794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and processor from local\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./saved_model/')\n",
    "processor = Wav2Vec2Processor.from_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ba9f48-8065-40a3-b9ba-119b82c4047e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a3fad1af7245fca39b4588b4b5188b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1818 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0d534dabec47f09472654e5d9a2230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b93b527f2c248e584189a1323d30223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f30e99923044e5bafc9f01d636330d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset variable if loading the saved model\n",
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])\n",
    "\n",
    "# check the format of the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a23207-f990-45cf-8aae-a531b14fa107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorCTCWithPadding(processor=Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"Wav2Vec2Processor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='./processor/', vocab_size=32, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]', 'additional_special_tokens': [AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True)]}), padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setup trainer datacollator\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "# define the data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f60f66-2eb1-4848-a758-18f38b701fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c8bb10-814f-4a44-9455-947180880ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./ckpt',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=20,#30\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=200,#500,\n",
    "  eval_steps=200,#500,\n",
    "  logging_steps=200,#500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=3,\n",
    "  push_to_hub=False,\n",
    ")\n",
    "\n",
    "# defining the training class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a34407-8090-4b8f-9730-80258a162aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./ckpt/checkpoint-3200).\n",
      "You are resuming training from a checkpoint trained with 4.17.0 of Transformers but your current version is 4.18.0. This is not recommended and could yield to errors or unwanted behaviors.\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1796\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4500\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 14\n",
      "  Continuing training from global step 3200\n",
      "  Will skip the first 14 epochs then the first 50 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a906b1ae063641138dbfd80fda5932bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4189' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4189/4500 19:37 < 06:11, 0.84 it/s, Epoch 18.61/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.453289</td>\n",
       "      <td>0.199876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.432563</td>\n",
       "      <td>0.203173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.812758</td>\n",
       "      <td>0.197610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.459810</td>\n",
       "      <td>0.196167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-3400\n",
      "Configuration saved in ./ckpt/checkpoint-3400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-3400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-3400/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-3600\n",
      "Configuration saved in ./ckpt/checkpoint-3600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-3600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-3600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-3800\n",
      "Configuration saved in ./ckpt/checkpoint-3800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-3800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-3800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-3200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-4000\n",
      "Configuration saved in ./ckpt/checkpoint-4000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-4000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-4000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-3400] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "# continue the finetuning\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d85d9d63-7452-4e05-9356-ebd3fc16c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_model\n",
      "Configuration saved in ./saved_model/config.json\n",
      "Model weights saved in ./saved_model/pytorch_model.bin\n",
      "Feature extractor saved in ./saved_model/preprocessor_config.json\n",
      "Feature extractor saved in ./processor/preprocessor_config.json\n",
      "tokenizer config file saved in ./processor/tokenizer_config.json\n",
      "Special tokens file saved in ./processor/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save model to local directory\n",
    "trainer.save_state()\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# save the processor\n",
    "processor.save_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b6134-0fd1-4561-a65d-302316458490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b906cf6-9eee-43f8-9dc3-88ff17245ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a2e4907-ceb4-4ad3-a4f2-8613043b59b5",
   "metadata": {},
   "source": [
    "# Code to do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4987d98a-2450-4b3a-9548-23da4218b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and processor from local\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./saved_model/')\n",
    "processor = Wav2Vec2Processor.from_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb80917-d459-42a3-8a2f-c842c33eaa51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe7f36-91a1-4b18-8b35-989de6e641e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafd0f04-f6ca-44eb-b614-5bda437f4a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663daef4b19544868a1c2f75854ae9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1818 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7596c74e794743b68f0e77b2349766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94b5aed58174eb38b9f13f27da6311e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09686c2ddc8c46b38ec4f0070fbb457c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset variable if loading the saved model\n",
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])\n",
    "\n",
    "# check the format of the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba27be-e1f8-4076-8865-2b2e742fca36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b5761-803b-4143-8892-774bf45a0951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9b350f-1a52-44e4-aeb5-b781965d0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTATION\n",
    "def map_to_result_gpu(batch):\n",
    "    model.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb26692-10a1-42db-b5b1-3b94082e7543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5815152bb24cad8a5ba34453033080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dev = dataset[\"dev\"].map(map_to_result_gpu, remove_columns=dataset[\"dev\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381f6405-2ee6-453a-98f0-0ad8914f062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63642b2cd57c462287ea0a312a68cdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_test = dataset[\"test\"].map(map_to_result_gpu, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bb974-a1e5-40d8-8fbd-31ae7532c90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d53b0a-8c25-4084-9de4-3414a1c5bfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OOW INCREASE ONE THREE',\n",
       " 'YOUR LAST AFFIRM THE AIR LANE IS THREE THOUSAND FEET AND UP TO FOUR THOUSAND FOURTY SIX THOUSAND',\n",
       " 'CONCUR SURFACE THREAT WARNING NOW RED',\n",
       " 'OOW INCREASE TWO SEVEN OOW COME RIGHT THREE FOUR FIVE',\n",
       " 'AGUN ALARM MISSILE AT TWO MILES AGUN ALARM MISSILE AT TWO MILES']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49fa18-0b82-40fd-9282-ddec0ce513f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4925b451-12b1-422a-a8d8-07cc0ef83e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT MINUTES ONE THREE ONE FIVE O F R FIVE DECIMAL FOUR MILES OUT FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC ALFA FOUR QUEBEC ENCOUNTER MISFIRE SAFETY BEARING ZERO ZERO ZERO INTEND TO CONDUCT MISFIRE DRILL BEARING ZERO ZERO ZERO'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dev['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e80758b-d9b9-47a3-9bfd-ab93cdb13adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT MINUTES ONE THREE ONE FIVE O FFA FIVE DECIMAL FOUR OUT FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC ALFA FOUR QUEBEC ENCOUNTER MISFIRE SAFETY BEARING ZERO ZERO ZERO INTEND TO CONDUCT MISSFIRE GRAL BEARING ZERO ZERO ZERO'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dev['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b5206-d1e3-4702-b1f1-a927578043a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fc989-5637-450b-934f-0c79fc04791c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac566724-82d8-4e57-942f-30acb300b03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OOW INCREASE ONE THREE'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad4e6f9-4398-48cb-9090-4a6fd9b06feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OW INCREASE ONE THREE'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50101f5-4e1d-4fff-b5e8-e2bbb026218f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "621cbd11-42be-496c-9702-0c82dff58e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de91d7fb-15f5-406a-93de-bc33663de992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation WER: 0.177\n",
      "Test WER: 0.236\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation WER: {:.3f}\".format(wer_metric.compute(predictions=results_dev[\"pred_str\"], references=results_dev[\"text\"])))\n",
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results_test[\"pred_str\"], references=results_test[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497f9b7-9c19-4d20-8233-3714931fb5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66cf2a-db01-424e-93c7-ae0c1897e920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e33370-63a2-468b-abb6-0ce5fcfcd7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75510b-337f-4892-bf0b-a33bec761cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ daniel test START ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ea1b9-3f4d-4ad2-a711-694ada39e845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e882dbd-4699-4587-a80d-f7ee1baa5817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003b046d5bef48b8ab4200ca43e37547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # define evaluation metric\n",
    "# wer_metric = load_metric(\"wer\")\n",
    "\n",
    "# def show_random_elements(dataset, num_examples=10):\n",
    "#     assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "#     picks = []\n",
    "#     for _ in range(num_examples):\n",
    "#         pick = random.randint(0, len(dataset)-1)\n",
    "#         while pick in picks:\n",
    "#             pick = random.randint(0, len(dataset)-1)\n",
    "#         picks.append(pick)\n",
    "    \n",
    "#     df = pd.DataFrame(dataset[picks])\n",
    "#     display(HTML(df.to_html()))\n",
    "\n",
    "# results_test = dataset[\"test\"].map(map_to_result_gpu, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fda3ab4c-28c2-4c60-90ca-328d2708d87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LAN BAO HAI INFORMATION FOR YOU BEARING FROM YOU TWO SEVEN EIGHT DEGREES TWO SEVEN EIGHT DEGREES DISTANCE THREE DECIMAL ONE NAUTICAL MILE THREE DECIMAL ONE NAUTICAL MILE DUBAI ANGEL VESSEL DUBAI ANGEL WILL CROSS AHEAD OF YOU GOING EASTBOUND OVER'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "459d7962-7c79-4c02-b5d9-4cc3d6a96ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B HI INTURMAON BEARING FOM YOU TWO SEVEN HDCREASE TWO SEVEN HDCREASE DISTANCE THREE DEIMNAL ONE NAUTICAL MILE TRY DECIMAL ONE NAUTICAL MILES BY ANJL V DOVE HANZ CROSE AHEAD GOING DEAST ON OVER'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_test['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42e03b8a-e013-469b-9287-f848dd7f35d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.928\n"
     ]
    }
   ],
   "source": [
    "# print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results_test[\"pred_str\"], references=results_test[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779a4ea-61f4-4c06-8b35-6498153f8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random_elements(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ea287-efeb-4e7b-996e-c8052dbd327c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35064d06-228d-42e5-a613-1a75a1df2713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41acf788-5276-4636-a847-6754111a10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- daniel test ends -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80aaf4b-8102-4607-a979-a63a83cac958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0beaf5-5134-4250-8373-465dc3596880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "537e946d-7eb1-4ebc-a30d-33b5a4d6db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e93e5f6a-df91-48b8-8693-61c12af9edda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THIS IS PWO ROGER COMMAND PWO PERMISSION UPGRADE WEAPON EIGHT CORRECTION PERMISSION UPGRADE WEAPON READINESS STATE TWO CORRECTION A GUN READINESS STATE TWO COMMAND</td>\n",
       "      <td>THIS IS PWO ROGER COMMAND PWO PERMISSION TO UPGRADE WEAPON A CORRECTION PERMISSION TO UPGRADE WEAPON READINESS STATE TWO CORRECTION A GUN READINESS STATE TWO COMMAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROGER WEAPS PWO TRANSMIT M F R ALL ROUND TRAININFICUANCY</td>\n",
       "      <td>ROGER WEAPS PWO TRANSMIT M F R ALL ROUND TRAINING FREQUENCY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT OUT</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STATION PWO SURFACE INVESTIGATE U B [UNK] ZERO THREE ZERO BEARING [UNK] EIGHT DECIMAL ORRETION BEARING ZERO ZERO ZERO RANGE EIGHT DECIMAL NINE</td>\n",
       "      <td>ALL STATION PWO SURFACE INVESTIGATE U B [UNK] ZERO THREE ZERO BEARING [UNK] EIGHT DECIMAL CORRECTION BEARING ZERO ZERO ZERO RANGE EIGHT DECIMAL NINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THREE MILES</td>\n",
       "      <td>THREE MILES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SURFACE THREAD WARNING YELLOW TPS OW PWO INFORM ONCE [UNK] READY TO HAD OVER OPS CONTROL THIS IS PWO STEERING ZERO ZERO ONE SPEED TEN I HAVE OPS CONTROL OW PWO</td>\n",
       "      <td>SURFACE THREAT WARNING YELLOW TPS OOW PWO INFORM ONCE [UNK] READY TO HAND OVER OPS CONTROL THIS IS PWO STEERING ZERO ZERO ONE SPEED TEN I HAVE OPS CONTROL OOW PWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INTEND TO CONDUCT P A C ONE NAUTICAL MILES SOUTH OF STARTEX INTEND [UNK] FIRST ROUND OUT AT ONE ZERO THREE FIVE</td>\n",
       "      <td>INTEND TO CONDUCT P A C ONE NAUTICAL MILES SOUTH OF STARTEX INTEND FIRST ROUND OUT AT ONE ZERO THREE FIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[UNK] FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC</td>\n",
       "      <td>ALFA FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WEAPS PWO UPGRADE A GUN READINESS TO STATE TWO OOOW [UNK] PWO INCREASE TWO SEVEN  EW PWO CHECK WHART THE EW LION ZERO ZERO ZERO TWO</td>\n",
       "      <td>WEAPS PWO UPGRADE A GUN READINESS TO STATE TWO OOW [UNK] PWO INCREASE TWO SEVEN [UNK] EW PWO WHAT'S THE EW LINE [UNK] ZERO ZERO TWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PERMISSION TO UPGRADE A GUN READINESS TO STATE TWO WEAPS PWO UPGRADE A GUN READINESS T STATE TWO ALL IN PWO ACCESS  IL BE REACHING [UNK]</td>\n",
       "      <td>PWO PERMISSION TO UPGRADE A GUN READINESS TO STATE TWO WEAPS PWO UPGRADE A GUN READINESS TO STATE TWO ALL IN PWO ASSESS WE WILL BE REACHING [UNK]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e43e9-493b-4e16-889d-4d375959409c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d3bd99e-4f3c-47d4-b5c5-74adc35ac1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PWO BROF TRANSMIT F CR</td>\n",
       "      <td>PWO APPROVE TRANSMIT FCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENGAGE [UNK] P A C TRACK AT [UNK] THREE MILES OW PWO PWO ROGER A GUN CEASE FIRING WEAPS PWO ROGER PWOW HAL DOWN FLAG BRAVO</td>\n",
       "      <td>ENGAGE [UNK] P A C TRACK AT [UNK] THREE MILES OOW PWO PWO ROGER A GUN CEASE FIRE WEAPS PWO ROGER PWO OOW HAUL DOWN FLAG BRAVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMMAND THIS IS PWO K [UNK] FOR THIS STENARIO H F F S TWO HAS AREADY BEEN INSTROCTED TO POSIT TAKE UP POSITION IN EMBIRING POSITION THREE MILES [UNK] EAST OF J R IN THIS CASE [UNK] OWN FORCE ALFA FOUR QUEBEC WI DIRECT FOXTROT ONE DELTA TO CONDUCT DIVERSION FOR QUEBEC FOXTROT BEARING [UNK] ZERO TWO FIVE RANGE SEVEN MILES</td>\n",
       "      <td>COMMAND THIS PWO K [UNK] FOR THIS SCENARIO THREE FFS TWO HAVE ALREADY BEEN INSTRUCTED TO [UNK] TAKE UP POSITION EMPIRE POSITION THREE MILE [UNK] EAST OF JR THIS CASE [UNK] OWN FORCE ALPHA FOUR QBAT WILL DIRECT [UNK] FOXTORT ONE DETLA TO CONDUCT DIVSERION FOR QBAT FOXTROT BEARING [UNK] ZERO TWO FIVE RANGE SEVEN MILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THREE ZERO ZERO TRANSMIT F C R OOW COME RIGHT THREE TWO ZERO</td>\n",
       "      <td>THREE ZERO ZERO TRANSMIT F C R OOW COME RIGHT THREE TWO ZERO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A GUN COVER ZERO ZERO SEVEN THREE FCSCORRECTION A GUN COVER ZERO ZERO NINE FOUR</td>\n",
       "      <td>AGUN COVER ZERO ZERO SEVEN THREE CORRECTION AGUN COVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALL GUNERY POSITION GUNEARY BROAD CAST POLICY SURFACE AND AIR A GUN PROVIDE MOSE ALFHA LOADING BRAVO GUN PROVIDE ONE BOX HA H IT GJ F M J PROVIDE ONE BOX TWO FIVE ZERO ROUNDS LOOK OUT ALL ROUND LOOKOUT ROUTINE STANDARD FOUR FIVEALL GUNERY POSITION STAND TWO FCS</td>\n",
       "      <td>ALL GUNNERY POSITIONS GUNNERY BROADCAST POLICY SURFACE AND AIR AGUN PROVIDE MODE ALPHA LOADING BRAVO GUN PROVIDE ONE BOX H E I T G P M G PROVIDE ONE BOX TWO FIVE ZERO ROUNDS LOOKOUT ALL ROUND LOOKOUT ROUTINE STANDARD FOUR FIVE ALL GUNNERY POSITIONS STAND TO FCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR FQU QUEBEC A GUN EN COUNTER STOPAGE E T C  THREE U WAS INTENT TO REATURN TO BASE FOR A GUN RETIFICATION WILL BE UNLOADING REMAINING A GUN ROUNDS AND PROCEEDING BEC FOR PROCEDING BACK TO BASE FOR A GUN RETIFICATIONT FOXTROT ONE DELTA ON OVER</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR Q QUEBEC A GUN ENCOUNTER STOPPAGE E T C THREE HOURS INTEND TO RETURN TO BASE FOR A GUN RECTIFICATION WILL BE UNLOADING REMAINING A GUN ROUNDS AND PROCEEDING BACK FOR PROCEEDING BACK TO BASE FOR A GUN RECTIFICATION FOXTROT ONE DELTA OVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBAT [UNK] COMPLETED P A C FIRING PROCEEDING FOR J R FIRING STAND BY FOR FOR THE SIRA FOXTROT ONE DELTA THIS IS ALFA FOUR QEBAT FOR YOU THE HEAD[UNK] SOUTH TO ALLOCATED POSITION UNIFORM ALFA TWO THREE THREE FOXTRT NE HELSA OVER</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC [UNK] COMPLETED P A C FIRING [UNK] PROCEEDING FOR J R FIRING STAND BY FOR FURTHER SITREP FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC FOR YOU TO [UNK] HEAD [UNK] SOUTH TO ALLOCATED POSITION UNIFORM ALFA TWO THREE THREE FOXTROT ONE DELTA OVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PWO ROGER OW PWO PUT FLACG BRAVO AT DIP COM SPWO CHECK [UNK] N O C INFORM OF N REQUESTED SIWERE AND INTEL ROGER [UNK] CALL N O C AGAIN MINUTES ONE ZERO SHIPB COMMENCE [UNK] P A C FIRING MINUTES ONE ZERO ONE FIVE</td>\n",
       "      <td>PWO ROGER OOW PWO PUT FLAG BRAVO AT DIP COMMS PWO CHECK [UNK] N O C INFORM [UNK] OF AND REQUESTED SITREP AND INTEL ROGER [UNK] CALL N O C AGAIN MINUTES ONE ZERO SHIP WILL COMMENCES [UNK] P A C FIRING MINUTES ONE ZERO ONE FIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ALL STATION TIME MINUTES TWO ZERO COMMENCES COMMS CHECK OER THIS IS PWO LOWD AND CLEAR OVER THIS IS PWO ROGER ALL STATION THIS IS PWO MISSION BRIEF IS TO CONDUCT P A C FIRING BEFORE J R S S FIRING LUW OF TOP GUN RULES AFTER ONCE AFTER WE COMPLETED J R S S FOXTROT ONE DELTA WILL CONDUCT J R S S FIRING</td>\n",
       "      <td>ALL STATION TIME MINUTUE TWO ZERO COMMENCES COMMS CHECK OVER THIS IS PWO LOUD AND CLEAR OVER THIS IS PWO ROGER ALL STATION THIS IS PWO MISSION BRIEF IS TO CONDUCT P A C FIRING BEFORE J R S S FIRING IN VIEW OF TOP GUN RULES AFTER ONCE AFTER WE COMPLETED J R S S FOXTROT ONE DELTA WILL CONDUCT HER J R S S FIRING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbaa772-71ef-4e1c-becd-c6477a89aac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07e3feb2-7471-429e-8de0-aba3cced0153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F F O X X T R R O O T T [PAD] | | O N N E E [PAD] | | D E E L L [PAD] [PAD] T [PAD] A A [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | T T H I I S [PAD] [PAD] | | [PAD] I I [PAD] S [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | | A A L L [PAD] F F [PAD] A A [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | F F O U U R R [PAD] [PAD] [PAD] [PAD] [PAD] | | Q Q Q U [PAD] E E [PAD] [PAD] B B E E C [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | S H H O O [PAD] [PAD] T [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | M M I N N U U U T T E [PAD] S [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | O O N N E E | T H H R E E [PAD] [PAD] [PAD] E E [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | O O N E E [PAD] [PAD] | | F F I I V V E E [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | O O O O [PAD] | [PAD] [PAD] F [PAD] F [PAD] [PAD] [PAD] A A [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | F F I V E [PAD] [PAD] | D E E [PAD] C [PAD] I I M M A A L L [PAD] [PAD] [PAD] | F F O U U R R [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | [PAD] O O U U T T [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | F O O X T T R R O O T [PAD] | | O O N E E E [PAD] [PAD] | D D E L L [PAD] [PAD] T T A A [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | T H H I S [PAD] | | [PAD] I S [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] A A L L L [PAD] [PAD] F [PAD] [PAD] A A [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | F O O U U R R [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | Q Q U [PAD] E E [PAD] B B E E C [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] A L [PAD] F [PAD] A A [PAD] [PAD] [PAD] | F F O U U R [PAD] [PAD] [PAD] [PAD] | Q Q U [PAD] E [PAD] B B E [PAD] C [PAD] [PAD] | | E N N [PAD] [PAD] [PAD] [PAD] C O O U U N N [PAD] T T [PAD] E R R [PAD] [PAD] [PAD] [PAD] [PAD] | M M I I [PAD] [PAD] S [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F F [PAD] I I R R R E E [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | S [PAD] A F F E E T Y Y [PAD] [PAD] [PAD] | B B E A A R R I I N N G G [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | | Z E E R R O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | Z Z E E R O O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | Z Z E E R O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | I I N N [PAD] T [PAD] E E N N [PAD] D [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | | T T [PAD] O O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | C O O N N [PAD] D D [PAD] U [PAD] C T T [PAD] [PAD] [PAD] [PAD] | M M I I S [PAD] S [PAD] [PAD] [PAD] F F [PAD] I I R R R E E [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] G R R [PAD] A L L [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | [PAD] B E E A A R I I N N G [PAD] [PAD] [PAD] [PAD] [PAD] | | Z E E R R O O [PAD] [PAD] [PAD] [PAD] | | Z E E R O [PAD] [PAD] [PAD] [PAD] | Z Z E R R O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(dataset[\"dev\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70d4be-9d45-4c3a-bddc-bc4f0de924e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf4e9a56-0730-406b-bf1d-d3f1ec5519b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] W W [PAD] [PAD] [PAD] [PAD] [PAD] | | I N [PAD] C C R R E E A A S S E [PAD] [PAD] [PAD] | | [PAD] O N N E E | | T T H R R E E [PAD] [PAD] E E [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(dataset[\"test\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52212dc8-3027-4665-8cbc-dad1a9b1c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
