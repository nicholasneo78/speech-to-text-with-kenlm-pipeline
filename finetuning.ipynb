{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c8a073-bd3d-4acf-82a9-0eaf3233cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  6 10:36:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.00    Driver Version: 470.82.00    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   56C    P8    14W /  N/A |    711MiB / 16125MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f649107-5002-4b2d-8c80-d202778a970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31ac71-4cf9-4629-a842-04f73e713db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484acc-e61c-4a78-a628-11ec4e832ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3775810-a515-4a53-81fb-c3fd9f7ae98e",
   "metadata": {},
   "source": [
    "# Code for finetuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb365a-ec07-4521-9642-e54eb5d581bb",
   "metadata": {},
   "source": [
    "## Convert the dataset imported from the pkl files into a DatasetDict object for training on the transformers later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a0a581c-e554-4d56-9ca8-7f688e8b2082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 1818\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# check the format of the DatasetDict\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f2305-501f-4b2e-b3cd-ad056bf14e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33f34440-5232-4e5c-84b1-cd6fcac6c850",
   "metadata": {},
   "source": [
    "## Preparing the feature extractor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eadb05ea-4893-4e52-87c1-e80ce5a5924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all characters available in the train and dev datasets\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6aab06f-b48e-4a9c-90da-d9339810c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract characters from train dataset\n",
    "vocabs_train = extract_all_chars(df_train)\n",
    "\n",
    "# extract characters from dev dataset\n",
    "vocabs_dev = extract_all_chars(df_dev)\n",
    "\n",
    "# create a union of all distinct letters in the training and the dev datasets\n",
    "vocab_list = list(set(vocabs_train) | set(vocabs_dev))\n",
    "\n",
    "# convert resulting list into an enumerated dictionary\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "# replace space with a more visible character |\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "# add the [UNK] and the [PAD] token\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "# vocabulary is completed, now save the vocabulary as a json file\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b5b7c37-79fb-4d47-ae1f-8bbfa93b2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE TOKENIZER\n",
    "# use the json file to instantiate an object of the Wav2Vec2CTCTokenizer class\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "# after the tokenizer object is created, the vocab.json file is not needed anymore, since the processor file will be created and the vocab.json will be there, hence can remove it\n",
    "os.remove('vocab.json')\n",
    "\n",
    "# PREPARING THE FEATURE EXTRACTOR\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "\n",
    "# wrap the feature extractor and tokenizer as a single Wav2VevProcessor class object\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d405364-5b31-41c1-b7e4-438cde4c85ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66fa6b1-0757-4d98-b225-b1a20f9d9f59",
   "metadata": {},
   "source": [
    "## Further preparation of the dataset after defining the processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba63e14f-3c74-4261-ad91-29f543df992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660c7364aa2148c8ae3b0c1e9bef1342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1818 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc05beeb26d846b083d856adb49c3ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80dc24c-4b5e-4924-b5ea-2145867b1807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1818\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the format of the DatasetDict again with the required values needed for the training later\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f75680-6f0f-48fa-9c82-ed7d7fce0a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75548e34-0e28-4abf-a11e-ecc764d51078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13cf0f1d-b570-4284-b7b4-dd05b804eb0c",
   "metadata": {},
   "source": [
    "## Get the audio sample distribution (16k sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683a09a7-6f42-452d-8c32-73215430822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1818it [02:25, 12.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# make a list to get the list of audio length of all the training data\n",
    "audio_length_list = []\n",
    "for idx, item in tqdm(enumerate(dataset['train'])):\n",
    "    audio_length_list.append(dataset['train'][idx]['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62bc2c99-5284-4fc2-8195-c656017496db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the distribution of the audio sample\n",
    "data_dist = pd.Series(audio_length_list)\n",
    "\n",
    "data_dist.plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Number of inputs')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a55711d-a263-43ce-9ab6-0e0b6c2055b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the distribution of the audio length (in 16k sample)\n",
    "data_dist = pd.Series([audio_length/16000 for audio_length in audio_length_list])\n",
    "\n",
    "data_dist.plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Length of Audio')\n",
    "plt.ylabel('Number of inputs')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98abc9d1-dc1f-448c-a6b5-e4eb95815e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821cddc8083141fa8e78c09c13d8ed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f756be25-978c-4237-9e92-b43b09f37bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186354b-7852-43ee-85c3-0f6964c9d8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7870d-62b6-4954-b8fb-ba9300fae166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aeff6b4-f7e4-47a5-bbde-c0b42b9535e2",
   "metadata": {},
   "source": [
    "## Set up Trainer class for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286b565-f512-4474-9ab6-9975407bf5db",
   "metadata": {},
   "source": [
    "In contrast to the common data collators, this data collator treats the input_values and labels differently and thus applies to separate padding functions on them (again making use of Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function. Analogous to the common data collators, the padding tokens in the labels with -100 so that those tokens are not taken into account when computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cadd8a1f-7a42-49ff-b1c0-037f8523bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76225a7d-cf69-47d0-9ea4-aae1db44e209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorCTCWithPadding(processor=Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='', vocab_size=32, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}), padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db4170cf-8687-4a1d-a3c7-de8a6e8613fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c3c4d8593a4dd3bef1efa4f80da944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8548ef2-2db7-479a-b7c8-4ca33274316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0757e2-f60e-402d-b8a4-fec215c06cf2",
   "metadata": {},
   "source": [
    "The model will return a sequence of logit vectors:\n",
    "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
    "\n",
    "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1fa228d-336a-44ea-9be3-0f70fdb25ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aff7dd-639b-45dd-b82a-e6854962e930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e60086af-4e19-4bb6-a2e8-31c1dda8dd5b",
   "metadata": {},
   "source": [
    "Now, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f5034fa-eef4-4fe3-97fd-b346555cc345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d364f7e29b466da0f4a03d9c47fea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py:357: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59bfc3ee843462883aee853974c9275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained wav2vec2 checkpoint\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e4949-7bef-4bfd-b6a5-dffea32272a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c35e3bef-f9da-499a-8ae5-4ee66698dcba",
   "metadata": {},
   "source": [
    "The first component of Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretrainind and as stated in the [paper](https://arxiv.org/abs/2006.11477) does not need to be fine-tuned anymore. \n",
    "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part.  \n",
    "\n",
    "In a final step, we define all parameters related to training. \n",
    "To give more explanation on some of the parameters:\n",
    "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
    "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Timit dataset and might be suboptimal for other speech datasets.\n",
    "\n",
    "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
    "\n",
    "During training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. It allows you to also play around with the demo widget even while your model is still training.\n",
    "\n",
    "**Note**: If one does not want to upload the model checkpoints to the hub, simply set `push_to_hub=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af625440-6d50-4e4b-8444-542a8fba107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./ckpt',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,#30\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=200,#500,\n",
    "  eval_steps=200,#500,\n",
    "  logging_steps=200,#500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=3,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "483b3488-d592-4617-9fe0-6908e7ab0788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# defining the training class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f33b1b71-5547-4ba1-be71-bd7f77762b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1796\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 43:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>11.266900</td>\n",
       "      <td>3.609910</td>\n",
       "      <td>0.999588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.339200</td>\n",
       "      <td>3.145334</td>\n",
       "      <td>0.999588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.393100</td>\n",
       "      <td>3.096863</td>\n",
       "      <td>0.999588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.783400</td>\n",
       "      <td>1.405506</td>\n",
       "      <td>0.818463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.785900</td>\n",
       "      <td>0.934948</td>\n",
       "      <td>0.598084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.013300</td>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.323408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.780300</td>\n",
       "      <td>0.509189</td>\n",
       "      <td>0.320523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.620200</td>\n",
       "      <td>0.444620</td>\n",
       "      <td>0.259221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.524300</td>\n",
       "      <td>0.506075</td>\n",
       "      <td>0.237070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.438669</td>\n",
       "      <td>0.218937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>0.427522</td>\n",
       "      <td>0.214403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-200\n",
      "Configuration saved in ./ckpt/checkpoint-200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-400\n",
      "Configuration saved in ./ckpt/checkpoint-400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-4200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-600\n",
      "Configuration saved in ./ckpt/checkpoint-600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-4400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-800\n",
      "Configuration saved in ./ckpt/checkpoint-800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1000\n",
      "Configuration saved in ./ckpt/checkpoint-1000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1200\n",
      "Configuration saved in ./ckpt/checkpoint-1200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1400\n",
      "Configuration saved in ./ckpt/checkpoint-1400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1600\n",
      "Configuration saved in ./ckpt/checkpoint-1600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1800\n",
      "Configuration saved in ./ckpt/checkpoint-1800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2000\n",
      "Configuration saved in ./ckpt/checkpoint-2000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2200\n",
      "Configuration saved in ./ckpt/checkpoint-2200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=2.3512372902764214, metrics={'train_runtime': 2595.9164, 'train_samples_per_second': 6.919, 'train_steps_per_second': 0.867, 'total_flos': 1.677600751135104e+18, 'train_loss': 2.3512372902764214, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start the finetuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d1b96-1ac6-47db-a3b5-82a2e77d43ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b3739d1-a262-4f02-96ce-4de878cb138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_model\n",
      "Configuration saved in ./saved_model/config.json\n",
      "Model weights saved in ./saved_model/pytorch_model.bin\n",
      "Feature extractor saved in ./saved_model/preprocessor_config.json\n",
      "Feature extractor saved in ./processor/preprocessor_config.json\n",
      "tokenizer config file saved in ./processor/tokenizer_config.json\n",
      "Special tokens file saved in ./processor/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save model to local directory\n",
    "trainer.save_state()\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# save the processor\n",
    "processor.save_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf5964-6736-4027-9dbc-79d7d67074cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b22775-bdb6-465c-8e9d-957e2a90a322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd3311c-31b3-4f78-b44c-3399a3b79c60",
   "metadata": {},
   "source": [
    "## Code to resume training/finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa746f6-6cd1-473b-9019-18eb12c59794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./saved_model/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"./saved_model/\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 31,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file ./saved_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
      "\n",
      "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at ./saved_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
      "loading feature extractor configuration file ./processor/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"Wav2Vec2Processor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file ./processor/vocab.json\n",
      "loading file ./processor/tokenizer_config.json\n",
      "loading file ./processor/added_tokens.json\n",
      "loading file ./processor/special_tokens_map.json\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# load the model and processor from local\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./saved_model/')\n",
    "processor = Wav2Vec2Processor.from_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ba9f48-8065-40a3-b9ba-119b82c4047e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2722951f9b94ea2ac2372fd60bf5355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1818 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e11c0dc5b694dd9944016898c7c4848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04ffb63ce634000aeee6989ec0ccb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033345868c9e401ba3c81a161c93d1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset variable if loading the saved model\n",
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])\n",
    "\n",
    "# check the format of the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01a23207-f990-45cf-8aae-a531b14fa107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorCTCWithPadding(processor=Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"Wav2Vec2Processor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='./processor/', vocab_size=32, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]', 'additional_special_tokens': [AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True)]}), padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setup trainer datacollator\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "# define the data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9f60f66-2eb1-4848-a758-18f38b701fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65c8bb10-814f-4a44-9455-947180880ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./ckpt',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=15,#30\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=200,#500,\n",
    "  eval_steps=200,#500,\n",
    "  logging_steps=200,#500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=3,\n",
    "  push_to_hub=False,\n",
    ")\n",
    "\n",
    "# defining the training class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94a34407-8090-4b8f-9730-80258a162aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./ckpt/checkpoint-2600).\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1796\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3375\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 11\n",
      "  Continuing training from global step 2600\n",
      "  Will skip the first 11 epochs then the first 125 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1276e5b76c4ab6ab0d62e9ca4dcd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 15:11, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>0.413688</td>\n",
       "      <td>0.207501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.403625</td>\n",
       "      <td>0.204822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.391378</td>\n",
       "      <td>0.198846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2800\n",
      "Configuration saved in ./ckpt/checkpoint-2800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-3000\n",
      "Configuration saved in ./ckpt/checkpoint-3000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-3000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-3200\n",
      "Configuration saved in ./ckpt/checkpoint-3200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-3200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-3200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3375, training_loss=0.07697533275462963, metrics={'train_runtime': 977.9378, 'train_samples_per_second': 27.548, 'train_steps_per_second': 3.451, 'total_flos': 2.516641835053144e+18, 'train_loss': 0.07697533275462963, 'epoch': 15.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continue the finetuning\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d85d9d63-7452-4e05-9356-ebd3fc16c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_model\n",
      "Configuration saved in ./saved_model/config.json\n",
      "Model weights saved in ./saved_model/pytorch_model.bin\n",
      "Feature extractor saved in ./saved_model/preprocessor_config.json\n",
      "Feature extractor saved in ./processor/preprocessor_config.json\n",
      "tokenizer config file saved in ./processor/tokenizer_config.json\n",
      "Special tokens file saved in ./processor/special_tokens_map.json\n",
      "added tokens file saved in ./processor/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "# save model to local directory\n",
    "trainer.save_state()\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# save the processor\n",
    "processor.save_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b6134-0fd1-4561-a65d-302316458490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a2e4907-ceb4-4ad3-a4f2-8613043b59b5",
   "metadata": {},
   "source": [
    "# Code to do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4987d98a-2450-4b3a-9548-23da4218b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./saved_model/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"./saved_model/\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 31,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file ./saved_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
      "\n",
      "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at ./saved_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
      "loading feature extractor configuration file ./processor/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"Wav2Vec2Processor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file ./processor/vocab.json\n",
      "loading file ./processor/tokenizer_config.json\n",
      "loading file ./processor/added_tokens.json\n",
      "loading file ./processor/special_tokens_map.json\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# load the model and processor from local\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./saved_model/')\n",
    "processor = Wav2Vec2Processor.from_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe190936-5eb7-4a69-8c59-335fecf7b54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dafd0f04-f6ca-44eb-b614-5bda437f4a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd26f60c26b94b0ab0b0d3c1b1eb587d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1818 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbc538c60cf48ba8ab07a61c9a3ff31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7961a7c71404478e820c14f586c1bf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792b55d25e844b4e90737b2a83972ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset variable if loading the saved model\n",
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])\n",
    "\n",
    "# check the format of the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba27be-e1f8-4076-8865-2b2e742fca36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b5761-803b-4143-8892-774bf45a0951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f9b350f-1a52-44e4-aeb5-b781965d0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTATION\n",
    "def map_to_result_gpu(batch):\n",
    "    model.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efb26692-10a1-42db-b5b1-3b94082e7543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cea3e830b9490f8f88c28b914863f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dev = dataset[\"dev\"].map(map_to_result_gpu, remove_columns=dataset[\"dev\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "381f6405-2ee6-453a-98f0-0ad8914f062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567da213dc70454fb1117f0844e1cec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_test = dataset[\"test\"].map(map_to_result_gpu, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bb974-a1e5-40d8-8fbd-31ae7532c90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4925b451-12b1-422a-a8d8-07cc0ef83e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT MINUTES ONE THREE ONE FIVE O F R FIVE DECIMAL FOUR MILES OUT FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC ALFA FOUR QUEBEC ENCOUNTER MISFIRE SAFETY BEARING ZERO ZERO ZERO INTEND TO CONDUCT MISFIRE DRILL BEARING ZERO ZERO ZERO'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dev['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e80758b-d9b9-47a3-9bfd-ab93cdb13adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT MINUTES ONE THREE ONE FIVE  F  FIVE DECIMAL FOUR OUT FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC ALFA FOUR QUEBEC ENCOUNTER MIS FIRE SAFETY BEARING ZERO ZERO ZERO INTEND TO CONDUCT MISFIRE ERO BEARING ZERO ZERO ZERO'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dev['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fc989-5637-450b-934f-0c79fc04791c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac566724-82d8-4e57-942f-30acb300b03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OOW INCREASE ONE THREE'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ad4e6f9-4398-48cb-9090-4a6fd9b06feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OW INCREASE ONE THREE'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50101f5-4e1d-4fff-b5e8-e2bbb026218f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75510b-337f-4892-bf0b-a33bec761cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3abf3704-0946-4698-a510-aec291f5e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8a9968c-1b98-46da-a685-30043cece8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation WER: 0.162\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation WER: {:.3f}\".format(wer_metric.compute(predictions=results_dev[\"pred_str\"], references=results_dev[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c712cbcd-4802-4d18-a2cd-39dbc493b8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.222\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results_test[\"pred_str\"], references=results_test[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648b528-f48f-4500-b81b-9172bb3b80de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0beaf5-5134-4250-8373-465dc3596880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "537e946d-7eb1-4ebc-a30d-33b5a4d6db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e93e5f6a-df91-48b8-8693-61c12af9edda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THIS IS PWO ROGER COMMAND PWO PERMISSION UPGRADE WEAPON EIGHT CORRECTION PEMISSION UPGRADE WEAPON READINESS STATE TWO CORRECTION A GUN READINESS STATE TWO COMMAND</td>\n",
       "      <td>THIS IS PWO ROGER COMMAND PWO PERMISSION TO UPGRADE WEAPON A CORRECTION PERMISSION TO UPGRADE WEAPON READINESS STATE TWO CORRECTION A GUN READINESS STATE TWO COMMAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROGER WEAPS PWO TRANSMIT M L R ALL ROUND CRAINING FREQUENCY</td>\n",
       "      <td>ROGER WEAPS PWO TRANSMIT M F R ALL ROUND TRAINING FREQUENCY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT OUT</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALL STATION PWO SURFACE INVESTIGATE U B # ZERO THREE ZERO BEARING # EIGHT DECIMAL CORRCTION BEARING ZERO ZERO ZERO RANGE EIGHT DECIMAL NINE</td>\n",
       "      <td>ALL STATION PWO SURFACE INVESTIGATE U B # ZERO THREE ZERO BEARING # EIGHT DECIMAL CORRECTION BEARING ZERO ZERO ZERO RANGE EIGHT DECIMAL NINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THREE MILES</td>\n",
       "      <td>THREE MILES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SURFACE THREAT WARNING YELLOW TPS OW PWO INFORM ONCE # READY TO HAND OVER OPS CONTROL THIS IS PWO STEARING ZERO ZERO ONE SPEED TEND I HAVE OPS CONTROL OW PWO</td>\n",
       "      <td>SURFACE THREAT WARNING YELLOW TPS OOW PWO INFORM ONCE # READY TO HAND OVER OPS CONTROL THIS IS PWO STEERING ZERO ZERO ONE SPEED TEN I HAVE OPS CONTROL OOW PWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INTEND TO CONDUCT P A C ONE NAUTICAL MILES SOUTH OF STARTEX INTEND FIRST ROUND OUT AT ONE ZERO THREE FIVE</td>\n",
       "      <td>INTEND TO CONDUCT P A C ONE NAUTICAL MILES SOUTH OF STARTEX INTEND FIRST ROUND OUT AT ONE ZERO THREE FIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AL FOXTROT ONE DELTA THIS IS ALPHA FOUR QUEBEC</td>\n",
       "      <td>ALFA FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WEAPS PWO UPGRADE A GUN READINESS STATE TWO OOW # PWO INCREASE TWO SEVEN EW PWO CHECK WHANTE THE EW LINON ZERO ZERO ZERO TWO</td>\n",
       "      <td>WEAPS PWO UPGRADE A GUN READINESS TO STATE TWO OOW # PWO INCREASE TWO SEVEN # EW PWO WHAT'S THE EW LINE # ZERO ZERO TWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PERMISSION TO UPGRADE A GUN READINESS STATE TWO WEAPS PWO UPGRADE A GUN READINESS STATE TWO ALL IN PWO ACCESS # WILL BE REACHING #</td>\n",
       "      <td>PWO PERMISSION TO UPGRADE A GUN READINESS TO STATE TWO WEAPS PWO UPGRADE A GUN READINESS TO STATE TWO ALL IN PWO ASSESS WE WILL BE REACHING #</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e43e9-493b-4e16-889d-4d375959409c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d3bd99e-4f3c-47d4-b5c5-74adc35ac1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PWO APPROVE TRANSMIT FCR</td>\n",
       "      <td>PWO APPROVE TRANSMIT FCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENGAGE # P A C TRACK AT # THREE MILES OW PWO PWO ROGER A GUN CEASE FIRING WEAPS PWO ROGER PWO W HAUL DOWN FLAG BRAVO</td>\n",
       "      <td>ENGAGE # P A C TRACK AT # THREE MILES OOW PWO PWO ROGER A GUN CEASE FIRE WEAPS PWO ROGER PWO OOW HAUL DOWN FLAG BRAVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMMAND THIS IS PWO K  FOR THIS IRRIAL CHE F SS TWO HAS READY BENINSTUCTED TO POTANG UP POSITION I ABIRING POSITION THREE MILES TE EAST OF J R IN THIS CASE # OWN FORCE ALFA FOUR QUEBEC WIL DIRECT # FOXTROT ONE DELTA TO CONDUCT DIVERSION FOR QUEBEC FOXTROT BEARING ZERO TWO FIVE RANGE SEVEN MILES</td>\n",
       "      <td>COMMAND THIS PWO K # FOR THIS SCENARIO THREE FFS TWO HAVE ALREADY BEEN INSTRUCTED TO # TAKE UP POSITION EMPIRE POSITION THREE MILE # EAST OF JR THIS CASE # OWN FORCE ALPHA FOUR QBAT WILL DIRECT # FOXTORT ONE DETLA TO CONDUCT DIVSERION FOR QBAT FOXTROT BEARING # ZERO TWO FIVE RANGE SEVEN MILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THREE ZERO ZERO TRANSMIT F CR OW COME RIGHT THREE TWO ZERO</td>\n",
       "      <td>THREE ZERO ZERO TRANSMIT F C R OOW COME RIGHT THREE TWO ZERO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGUN COVER ZERO ZERO SEVEN THREE FCS CORRECTION AGUN COVER ZERO ZERO NINE FOUR</td>\n",
       "      <td>AGUN COVER ZERO ZERO SEVEN THREE CORRECTION AGUN COVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALL GUNRY POSITION GUNNERY BROADCAST POLICY SURFACE AND AIR AGUN PROVIDE MODE ALPHA LOADING BRAVO GUN PROVIDE OUNE BOX H CH I T G FRE MCH PROVIDE ONE BOX TWO FIVE ZERO ROUNDS LOKOUT ALL ROUND LOKOUT ROUTINE STANDARD FOUR FIVE ALL GUNERY POSITIONS STAND TWO FCS</td>\n",
       "      <td>ALL GUNNERY POSITIONS GUNNERY BROADCAST POLICY SURFACE AND AIR AGUN PROVIDE MODE ALPHA LOADING BRAVO GUN PROVIDE ONE BOX H E I T G P M G PROVIDE ONE BOX TWO FIVE ZERO ROUNDS LOOKOUT ALL ROUND LOOKOUT ROUTINE STANDARD FOUR FIVE ALL GUNNERY POSITIONS STAND TO FCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFHA FOUR  QUEBEC A GUN ENCOUNTER STOPPAGE ET C THREE OUER INTEND TO RETURN TO BASE FOR A GUN RECTIFICATION WILL BE UNLOADING REMAINING A GUN ROUND AND PROCEEDING BEC FOR PROSIDIG BAC TO BASE FOR A GUN RECTIFCATO FOXTROT ONE DELTA OVER</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR Q QUEBEC A GUN ENCOUNTER STOPPAGE E T C THREE HOURS INTEND TO RETURN TO BASE FOR A GUN RECTIFICATION WILL BE UNLOADING REMAINING A GUN ROUNDS AND PROCEEDING BACK FOR PROCEEDING BACK TO BASE FOR A GUN RECTIFICATION FOXTROT ONE DELTA OVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FOXTROT NE DELTA THIS IS ALFA FOUR QUEBEC # COMPLETED P A C FIRING # PROCEEDING FOR J R FIRING STAND BY FOR HSERAC FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC FOR YOU THEAD SOUTH TO ILOCATED POSITION UNIFORM ALFA TWO THREE THREE FOXTROT ON SOUVER</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC # COMPLETED P A C FIRING # PROCEEDING FOR J R FIRING STAND BY FOR FURTHER SITREP FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC FOR YOU TO # HEAD # SOUTH TO ALLOCATED POSITION UNIFORM ALFA TWO THREE THREE FOXTROT ONE DELTA OVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PWO ROGER OOW PWO PUT FLAG BRAVO AT DIP COM SPWO CHECK # N O C INFORM OF AND REQUEST ED SIREP AND INDLNE ROGER # COL N O SAY AIN MINUTE ONE ZERO SHIP OCOMMENCES # P A C FIRING MINUTE ONE ZERO ONE FIVE</td>\n",
       "      <td>PWO ROGER OOW PWO PUT FLAG BRAVO AT DIP COMMS PWO CHECK # N O C INFORM # OF AND REQUESTED SITREP AND INTEL ROGER # CALL N O C AGAIN MINUTES ONE ZERO SHIP WILL COMMENCES # P A C FIRING MINUTES ONE ZERO ONE FIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ALL STATION TIME MINUTES TWO ZERO COMMENCES COMMS CHECK OVER THIS IS PWO LOUD AND CLEAR OVER THIS IS PWO ROGER ALL STATION THIS IS PWO MISSION BRIEF IS TO CONDUCT P A C FIRING BEFORE J R S S FIRING IN L OF TOP GUN ROUTS AFTER ONCE AFTER WE COMPLETED J R S S FOXTROT ONE DELTA WIL CONDUCT HER J R S S FIRING</td>\n",
       "      <td>ALL STATION TIME MINUTUE TWO ZERO COMMENCES COMMS CHECK OVER THIS IS PWO LOUD AND CLEAR OVER THIS IS PWO ROGER ALL STATION THIS IS PWO MISSION BRIEF IS TO CONDUCT P A C FIRING BEFORE J R S S FIRING IN VIEW OF TOP GUN RULES AFTER ONCE AFTER WE COMPLETED J R S S FOXTROT ONE DELTA WILL CONDUCT HER J R S S FIRING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbaa772-71ef-4e1c-becd-c6477a89aac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07e3feb2-7471-429e-8de0-aba3cced0153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F O X [PAD] T T R O O T T | | O N N E | | [PAD] [PAD] D D E E L L [PAD] [PAD] [PAD] T A A | | [PAD] [PAD] [PAD] [PAD] [PAD] T H H I S S [PAD] | [PAD] [PAD] [PAD] I I S S | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] A A L L [PAD] [PAD] [PAD] F A A A | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F O U U R R | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Q Q U U E E [PAD] [PAD] B E E C C | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] S S H O O O T T | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] M I I N N U T T E E S | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O N E | | T T H H R E E [PAD] [PAD] E | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O N E E | [PAD] [PAD] [PAD] [PAD] F I I V E E | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] [PAD] F [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F I I V E | | D D E C C [PAD] I I I M A A L L | | [PAD] [PAD] [PAD] F O O U U R R | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O U U T | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F O O X [PAD] T T R O O T | | O O N E E | [PAD] [PAD] [PAD] [PAD] D D E L L [PAD] [PAD] [PAD] T A A | | [PAD] [PAD] [PAD] [PAD] [PAD] T T H I I S | | [PAD] [PAD] I S | | [PAD] [PAD] [PAD] [PAD] [PAD] A A L L [PAD] [PAD] [PAD] [PAD] F F A A | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F O O U U R R | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Q U E E E [PAD] B E E C | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] A L L [PAD] F F A | | | [PAD] [PAD] F O O U U R | | [PAD] [PAD] [PAD] [PAD] Q U E E [PAD] B E E C | | | [PAD] E E N [PAD] [PAD] [PAD] [PAD] C C O O U N N N [PAD] [PAD] [PAD] T E R R | | [PAD] [PAD] [PAD] [PAD] [PAD] M I S S [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F F I I [PAD] [PAD] [PAD] R R E E E | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] S A A F E E [PAD] T T Y [PAD] | | [PAD] B E E A [PAD] R R I N N G G | | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Z E E R R O | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Z E E R R O | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Z Z E E R O | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] I N N [PAD] [PAD] T E E N N N D | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] T T O O O | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] C C O N N [PAD] [PAD] [PAD] D U U C T | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] M I S S [PAD] [PAD] [PAD] [PAD] [PAD] F F I [PAD] [PAD] [PAD] [PAD] R E E E | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] E R R R [PAD] O O | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] B E E A A R I I N G G | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Z E E R R O O | | [PAD] [PAD] [PAD] [PAD] Z E R R O O | [PAD] [PAD] [PAD] Z E E R R O O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(dataset[\"dev\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70d4be-9d45-4c3a-bddc-bc4f0de924e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf4e9a56-0730-406b-bf1d-d3f1ec5519b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O O O O O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] W W | | | [PAD] I N N C C C R R E A A S S E E | | [PAD] [PAD] [PAD] O N N E | | [PAD] T T H H R R E E [PAD] [PAD] E E [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(dataset[\"test\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52212dc8-3027-4665-8cbc-dad1a9b1c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
