{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c8a073-bd3d-4acf-82a9-0eaf3233cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 11 09:53:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.00    Driver Version: 470.82.00    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   58C    P8    16W /  N/A |    605MiB / 16125MiB |     11%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f649107-5002-4b2d-8c80-d202778a970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae9da1-f27d-48bc-8b9b-90426504cfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f189a1-856c-4928-9005-93b8874d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- daniel testing ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab31ac71-4cf9-4629-a842-04f73e713db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# data = []\n",
    "# with open('manifest.json') as f:\n",
    "#     for line in f:\n",
    "#         data.append(json.loads(line))\n",
    "        \n",
    "# with open('PRIV.txt', 'w+') as f:\n",
    "#     for i, j in enumerate(data):\n",
    "#         f.write(f\"PRIV_{i:03} {data[i]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99d3de7-62d6-4dfd-9ad6-6a7384399c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- daniel testing ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169194de-cd64-4f57-87d3-5a5151d8f87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068ab21-c48c-4546-86af-a19e44fb1ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa74b2-55dd-49ca-8462-be3120b9c589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484acc-e61c-4a78-a628-11ec4e832ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3775810-a515-4a53-81fb-c3fd9f7ae98e",
   "metadata": {},
   "source": [
    "# Code for finetuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb365a-ec07-4521-9642-e54eb5d581bb",
   "metadata": {},
   "source": [
    "## Convert the dataset imported from the pkl files into a DatasetDict object for training on the transformers later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0a581c-e554-4d56-9ca8-7f688e8b2082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 1818\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# check the format of the DatasetDict\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f2305-501f-4b2e-b3cd-ad056bf14e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33f34440-5232-4e5c-84b1-cd6fcac6c850",
   "metadata": {},
   "source": [
    "## Preparing the feature extractor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eadb05ea-4893-4e52-87c1-e80ce5a5924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all characters available in the train and dev datasets\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6aab06f-b48e-4a9c-90da-d9339810c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract characters from train dataset\n",
    "vocabs_train = extract_all_chars(df_train)\n",
    "\n",
    "# extract characters from dev dataset\n",
    "vocabs_dev = extract_all_chars(df_dev)\n",
    "\n",
    "# create a union of all distinct letters in the training and the dev datasets\n",
    "vocab_list = list(set(vocabs_train) | set(vocabs_dev))\n",
    "\n",
    "# convert resulting list into an enumerated dictionary\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "# replace space with a more visible character |\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "# add the [UNK], [PAD], bos and eos token\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "vocab_dict[\"<s>\"] = len(vocab_dict)\n",
    "vocab_dict[\"</s>\"] = len(vocab_dict)\n",
    "\n",
    "# make the useless vocabs as [UNK] in the end\n",
    "del vocab_dict[\"#\"]\n",
    "del vocab_dict[\"-\"]\n",
    "\n",
    "# renumber the dictionary values to fill up the blanks\n",
    "count = 0\n",
    "for key, value in vocab_dict.items():\n",
    "    vocab_dict[key] = count\n",
    "    count += 1\n",
    "    \n",
    "# vocabulary is completed, now save the vocabulary as a json file\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5b7c37-79fb-4d47-ae1f-8bbfa93b2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE TOKENIZER\n",
    "# use the json file to instantiate an object of the Wav2Vec2CTCTokenizer class\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\", bos_token='<s>', eos_token='</s>')\n",
    "\n",
    "# after the tokenizer object is created, the vocab.json file is not needed anymore, since the processor file will be created and the vocab.json will be there, hence can remove it\n",
    "os.remove('vocab.json')\n",
    "\n",
    "# PREPARING THE FEATURE EXTRACTOR\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "\n",
    "# wrap the feature extractor and tokenizer as a single Wav2VevProcessor class object\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fa6b1-0757-4d98-b225-b1a20f9d9f59",
   "metadata": {},
   "source": [
    "## Further preparation of the dataset after defining the processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63e14f-3c74-4261-ad91-29f543df992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80dc24c-4b5e-4924-b5ea-2145867b1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the format of the DatasetDict again with the required values needed for the training later\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f75680-6f0f-48fa-9c82-ed7d7fce0a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75548e34-0e28-4abf-a11e-ecc764d51078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13cf0f1d-b570-4284-b7b4-dd05b804eb0c",
   "metadata": {},
   "source": [
    "## Get the audio sample distribution (16k sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a09a7-6f42-452d-8c32-73215430822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list to get the list of audio length of all the training data\n",
    "audio_length_list = []\n",
    "for idx, item in tqdm(enumerate(dataset['train'])):\n",
    "    audio_length_list.append(dataset['train'][idx]['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc2c99-5284-4fc2-8195-c656017496db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution of the audio sample\n",
    "data_dist = pd.Series(audio_length_list)\n",
    "\n",
    "data_dist.plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Number of inputs')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55711d-a263-43ce-9ab6-0e0b6c2055b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution of the audio length (in 16k sample)\n",
    "data_dist = pd.Series([audio_length/16000 for audio_length in audio_length_list])\n",
    "\n",
    "data_dist.plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Length of Audio')\n",
    "plt.ylabel('Number of inputs')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abc9d1-dc1f-448c-a6b5-e4eb95815e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f756be25-978c-4237-9e92-b43b09f37bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186354b-7852-43ee-85c3-0f6964c9d8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7870d-62b6-4954-b8fb-ba9300fae166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aeff6b4-f7e4-47a5-bbde-c0b42b9535e2",
   "metadata": {},
   "source": [
    "## Set up Trainer class for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286b565-f512-4474-9ab6-9975407bf5db",
   "metadata": {},
   "source": [
    "In contrast to the common data collators, this data collator treats the input_values and labels differently and thus applies to separate padding functions on them (again making use of Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function. Analogous to the common data collators, the padding tokens in the labels with -100 so that those tokens are not taken into account when computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cadd8a1f-7a42-49ff-b1c0-037f8523bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76225a7d-cf69-47d0-9ea4-aae1db44e209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorCTCWithPadding(processor=Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='', vocab_size=32, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}), padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4170cf-8687-4a1d-a3c7-de8a6e8613fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8548ef2-2db7-479a-b7c8-4ca33274316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0757e2-f60e-402d-b8a4-fec215c06cf2",
   "metadata": {},
   "source": [
    "The model will return a sequence of logit vectors:\n",
    "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
    "\n",
    "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa228d-336a-44ea-9be3-0f70fdb25ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aff7dd-639b-45dd-b82a-e6854962e930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e60086af-4e19-4bb6-a2e8-31c1dda8dd5b",
   "metadata": {},
   "source": [
    "Now, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5034fa-eef4-4fe3-97fd-b346555cc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained wav2vec2 checkpoint - # load model from local instead of downloading from internet\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    #\"facebook/wav2vec2-base\",\n",
    "    './wav2vec2_base_model/',\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e3bef-f9da-499a-8ae5-4ee66698dcba",
   "metadata": {},
   "source": [
    "The first component of Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretrainind and as stated in the [paper](https://arxiv.org/abs/2006.11477) does not need to be fine-tuned anymore. \n",
    "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part.  \n",
    "\n",
    "In a final step, we define all parameters related to training. \n",
    "To give more explanation on some of the parameters:\n",
    "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
    "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Timit dataset and might be suboptimal for other speech datasets.\n",
    "\n",
    "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
    "\n",
    "During training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. It allows you to also play around with the demo widget even while your model is still training.\n",
    "\n",
    "**Note**: If one does not want to upload the model checkpoints to the hub, simply set `push_to_hub=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af625440-6d50-4e4b-8444-542a8fba107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./ckpt',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,#30\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=200,#500,\n",
    "  eval_steps=200,#500,\n",
    "  logging_steps=200,#500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "483b3488-d592-4617-9fe0-6908e7ab0788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# defining the training class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f33b1b71-5547-4ba1-be71-bd7f77762b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1796\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 41:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>29.554500</td>\n",
       "      <td>9.377405</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>14.998100</td>\n",
       "      <td>7.772687</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.890600</td>\n",
       "      <td>3.180052</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.294000</td>\n",
       "      <td>3.075372</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.529200</td>\n",
       "      <td>2.996717</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.280900</td>\n",
       "      <td>3.041261</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.028800</td>\n",
       "      <td>1.684989</td>\n",
       "      <td>0.895941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.045000</td>\n",
       "      <td>0.673530</td>\n",
       "      <td>0.505873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.497200</td>\n",
       "      <td>0.585380</td>\n",
       "      <td>0.377601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.124600</td>\n",
       "      <td>0.476169</td>\n",
       "      <td>0.308881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>0.444708</td>\n",
       "      <td>0.284154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-200\n",
      "Configuration saved in ./ckpt/checkpoint-200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-200/preprocessor_config.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-400\n",
      "Configuration saved in ./ckpt/checkpoint-400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-400/preprocessor_config.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-600\n",
      "Configuration saved in ./ckpt/checkpoint-600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-800\n",
      "Configuration saved in ./ckpt/checkpoint-800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1000\n",
      "Configuration saved in ./ckpt/checkpoint-1000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1200\n",
      "Configuration saved in ./ckpt/checkpoint-1200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1400\n",
      "Configuration saved in ./ckpt/checkpoint-1400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1600\n",
      "Configuration saved in ./ckpt/checkpoint-1600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-1800\n",
      "Configuration saved in ./ckpt/checkpoint-1800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2000\n",
      "Configuration saved in ./ckpt/checkpoint-2000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2200\n",
      "Configuration saved in ./ckpt/checkpoint-2200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=6.1912327423095705, metrics={'train_runtime': 2496.3366, 'train_samples_per_second': 7.195, 'train_steps_per_second': 0.901, 'total_flos': 1.677600751135104e+18, 'train_loss': 6.1912327423095705, 'epoch': 10.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start the finetuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d1b96-1ac6-47db-a3b5-82a2e77d43ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b3739d1-a262-4f02-96ce-4de878cb138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_model\n",
      "Configuration saved in ./saved_model/config.json\n",
      "Model weights saved in ./saved_model/pytorch_model.bin\n",
      "Feature extractor saved in ./saved_model/preprocessor_config.json\n",
      "Feature extractor saved in ./processor/preprocessor_config.json\n",
      "tokenizer config file saved in ./processor/tokenizer_config.json\n",
      "Special tokens file saved in ./processor/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save model to local directory\n",
    "trainer.save_state()\n",
    "trainer.save_model('./saved_model')\n",
    "# save the processor\n",
    "processor.save_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf5964-6736-4027-9dbc-79d7d67074cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b22775-bdb6-465c-8e9d-957e2a90a322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd3311c-31b3-4f78-b44c-3399a3b79c60",
   "metadata": {},
   "source": [
    "## Code to resume training/finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa746f6-6cd1-473b-9019-18eb12c59794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and processor from local\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./saved_model/')\n",
    "processor = Wav2Vec2Processor.from_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ba9f48-8065-40a3-b9ba-119b82c4047e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1753b7e6af7e4012ac45df714fdca4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1818 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8b3bf055d94c68ae866df2c11f450f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a73c5a20106492182c28ce581f058c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f27c664c3cd4eea949c101a7fa0f0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset variable if loading the saved model\n",
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])\n",
    "\n",
    "# check the format of the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a23207-f990-45cf-8aae-a531b14fa107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorCTCWithPadding(processor=Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"Wav2Vec2Processor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='./processor/', vocab_size=32, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}), padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setup trainer datacollator\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "# define the data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f60f66-2eb1-4848-a758-18f38b701fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b0276255194e3fbd756091acbbffc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c8bb10-814f-4a44-9455-947180880ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./ckpt',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=15,#30\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=200,#500,\n",
    "  eval_steps=200,#500,\n",
    "  logging_steps=200,#500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=3,\n",
    "  push_to_hub=False,\n",
    ")\n",
    "\n",
    "# defining the training class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94a34407-8090-4b8f-9730-80258a162aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./ckpt/checkpoint-2200).\n",
      "You are resuming training from a checkpoint trained with 4.17.0 of Transformers but your current version is 4.18.0. This is not recommended and could yield to errors or unwanted behaviors.\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1796\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3375\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 9\n",
      "  Continuing training from global step 2200\n",
      "  Will skip the first 9 epochs then the first 175 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74327192047e474fa7350cff0e90fd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 30:54, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.493890</td>\n",
       "      <td>0.234597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.424400</td>\n",
       "      <td>0.397666</td>\n",
       "      <td>0.209767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.405909</td>\n",
       "      <td>0.207810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.389415</td>\n",
       "      <td>0.192561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.342100</td>\n",
       "      <td>0.383866</td>\n",
       "      <td>0.191531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2400\n",
      "Configuration saved in ./ckpt/checkpoint-2400/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2400/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2400/preprocessor_config.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2600\n",
      "Configuration saved in ./ckpt/checkpoint-2600/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2600/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2600/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-2800\n",
      "Configuration saved in ./ckpt/checkpoint-2800/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-2800/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-2800/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-3000\n",
      "Configuration saved in ./ckpt/checkpoint-3000/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-3000/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2400] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 360\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./ckpt/checkpoint-3200\n",
      "Configuration saved in ./ckpt/checkpoint-3200/config.json\n",
      "Model weights saved in ./ckpt/checkpoint-3200/pytorch_model.bin\n",
      "Feature extractor saved in ./ckpt/checkpoint-3200/preprocessor_config.json\n",
      "Deleting older checkpoint [ckpt/checkpoint-2600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3375, training_loss=0.13333894178602432, metrics={'train_runtime': 1936.8754, 'train_samples_per_second': 13.909, 'train_steps_per_second': 1.742, 'total_flos': 2.516641835053144e+18, 'train_loss': 0.13333894178602432, 'epoch': 15.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continue the finetuning\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d85d9d63-7452-4e05-9356-ebd3fc16c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_model\n",
      "Configuration saved in ./saved_model/config.json\n",
      "Model weights saved in ./saved_model/pytorch_model.bin\n",
      "Feature extractor saved in ./saved_model/preprocessor_config.json\n",
      "Feature extractor saved in ./processor/preprocessor_config.json\n",
      "tokenizer config file saved in ./processor/tokenizer_config.json\n",
      "Special tokens file saved in ./processor/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save model to local directory\n",
    "trainer.save_state()\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# save the processor\n",
    "processor.save_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b6134-0fd1-4561-a65d-302316458490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b906cf6-9eee-43f8-9dc3-88ff17245ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a2e4907-ceb4-4ad3-a4f2-8613043b59b5",
   "metadata": {},
   "source": [
    "# Code to do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4987d98a-2450-4b3a-9548-23da4218b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./saved_model/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"./saved_model/\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 29,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file ./saved_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
      "\n",
      "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at ./saved_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
      "loading feature extractor configuration file ./processor/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"Wav2Vec2Processor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Didn't find file ./processor/added_tokens.json. We won't load it.\n",
      "loading file ./processor/vocab.json\n",
      "loading file ./processor/tokenizer_config.json\n",
      "loading file None\n",
      "loading file ./processor/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# load the model and processor from local\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./saved_model/')\n",
    "processor = Wav2Vec2Processor.from_pretrained('./processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb80917-d459-42a3-8a2f-c842c33eaa51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe7f36-91a1-4b18-8b35-989de6e641e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafd0f04-f6ca-44eb-b614-5bda437f4a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663daef4b19544868a1c2f75854ae9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1818 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7596c74e794743b68f0e77b2349766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94b5aed58174eb38b9f13f27da6311e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09686c2ddc8c46b38ec4f0070fbb457c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 1796\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'input_length', 'labels'],\n",
       "        num_rows: 334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset variable if loading the saved model\n",
    "# load the pickle data file\n",
    "with open('./pkl/magister_data_flac_16000_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "with open('./pkl/magister_data_flac_16000_dev.pkl', 'rb') as f:\n",
    "    df_dev = pickle.load(f)\n",
    "    \n",
    "with open('./pkl/magister_data_flac_16000_test.pkl', 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "    \n",
    "# make it into a DatasetDict Object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"dev\": Dataset.from_pandas(df_dev),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "# further preprocessing of the dataset for the transformers\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "# from the graph get the max sample length\n",
    "MAX_SAMPLE_LENGTH = 450000\n",
    "\n",
    "# filter out those longer duration videos (based on the histogram with the right tail minority)\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x < MAX_SAMPLE_LENGTH, input_columns=[\"input_length\"])\n",
    "\n",
    "# check the format of the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba27be-e1f8-4076-8865-2b2e742fca36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b5761-803b-4143-8892-774bf45a0951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9b350f-1a52-44e4-aeb5-b781965d0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTATION\n",
    "def map_to_result_gpu(batch):\n",
    "    model.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efb26692-10a1-42db-b5b1-3b94082e7543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77f5944440a41d092fa8176a3c55b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dev = dataset[\"dev\"].map(map_to_result_gpu, remove_columns=dataset[\"dev\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "381f6405-2ee6-453a-98f0-0ad8914f062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb97118f7a64a65a6648ee26c5bee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_test = dataset[\"test\"].map(map_to_result_gpu, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bb974-a1e5-40d8-8fbd-31ae7532c90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84d53b0a-8c25-4084-9de4-3414a1c5bfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OOW INCREASE ONE THREE',\n",
       " 'YOUR LAST AFFIRM THE AIR LANE IS THREE THOUSAND FEET AND UP TO FOUR THOUSAND FOURTY SIX THOUSAND',\n",
       " 'CONCUR SURFACE THREAT WARNING NOW RED',\n",
       " 'OOW INCREASE TWO SEVEN OOW COME RIGHT THREE FOUR FIVE',\n",
       " 'AGUN ALARM MISSILE AT TWO MILES AGUN ALARM MISSILE AT TWO MILES']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49fa18-0b82-40fd-9282-ddec0ce513f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4925b451-12b1-422a-a8d8-07cc0ef83e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT MINUTES ONE THREE ONE FIVE O F R FIVE DECIMAL FOUR MILES OUT FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC ALFA FOUR QUEBEC ENCOUNTER MISFIRE SAFETY BEARING ZERO ZERO ZERO INTEND TO CONDUCT MISFIRE DRILL BEARING ZERO ZERO ZERO'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dev['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e80758b-d9b9-47a3-9bfd-ab93cdb13adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOXTROT ONE DELTA THIS IS ALPFA FOUR QUEBEC SHOT MINUTE ONE THREE ONE FIVE O F R FIVE DECIMAL FOUR OUT FOXTROT ONE DELTA THIS IS ALPFA FOUR QUEBEC ALFA FOUR QUEBEC ENCOUNTER MISFIRE SAFETY BEARING ZERO ZERO ZERO INTEND TO CONDUCT MISFIRE ZEO BEARING ZERO ZERO ZERO'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dev['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b5206-d1e3-4702-b1f1-a927578043a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fc989-5637-450b-934f-0c79fc04791c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac566724-82d8-4e57-942f-30acb300b03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OOW INCREASE ONE THREE'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad4e6f9-4398-48cb-9090-4a6fd9b06feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OOW INCREASE ONE THREE'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50101f5-4e1d-4fff-b5e8-e2bbb026218f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621cbd11-42be-496c-9702-0c82dff58e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de91d7fb-15f5-406a-93de-bc33663de992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation WER: 0.153\n",
      "Test WER: 0.216\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation WER: {:.3f}\".format(wer_metric.compute(predictions=results_dev[\"pred_str\"], references=results_dev[\"text\"])))\n",
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results_test[\"pred_str\"], references=results_test[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497f9b7-9c19-4d20-8233-3714931fb5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66cf2a-db01-424e-93c7-ae0c1897e920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e33370-63a2-468b-abb6-0ce5fcfcd7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e75510b-337f-4892-bf0b-a33bec761cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ daniel test START ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ea1b9-3f4d-4ad2-a711-694ada39e845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e882dbd-4699-4587-a80d-f7ee1baa5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define evaluation metric\n",
    "# wer_metric = load_metric(\"wer\")\n",
    "\n",
    "# def show_random_elements(dataset, num_examples=10):\n",
    "#     assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "#     picks = []\n",
    "#     for _ in range(num_examples):\n",
    "#         pick = random.randint(0, len(dataset)-1)\n",
    "#         while pick in picks:\n",
    "#             pick = random.randint(0, len(dataset)-1)\n",
    "#         picks.append(pick)\n",
    "    \n",
    "#     df = pd.DataFrame(dataset[picks])\n",
    "#     display(HTML(df.to_html()))\n",
    "\n",
    "# results_test = dataset[\"test\"].map(map_to_result_gpu, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda3ab4c-28c2-4c60-90ca-328d2708d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "459d7962-7c79-4c02-b5d9-4cc3d6a96ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_test['pred_str'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42e03b8a-e013-469b-9287-f848dd7f35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results_test[\"pred_str\"], references=results_test[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9779a4ea-61f4-4c06-8b35-6498153f8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random_elements(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ea287-efeb-4e7b-996e-c8052dbd327c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35064d06-228d-42e5-a613-1a75a1df2713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41acf788-5276-4636-a847-6754111a10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- daniel test ends -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80aaf4b-8102-4607-a979-a63a83cac958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0beaf5-5134-4250-8373-465dc3596880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "537e946d-7eb1-4ebc-a30d-33b5a4d6db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e93e5f6a-df91-48b8-8693-61c12af9edda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THIS IS PWO ROGER COMMAND PWO PERMISSION UPGRADE WEAPON EIGHT CRRECTION PEMISSION UPGRADE WEAPON READINESS STATE TWO CORRECTION A GUN READINESS STATE TWO COMMAND</td>\n",
       "      <td>THIS IS PWO ROGER COMMAND PWO PERMISSION TO UPGRADE WEAPON A CORRECTION PERMISSION TO UPGRADE WEAPON READINESS STATE TWO CORRECTION A GUN READINESS STATE TWO COMMAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROGER WEAPS PWO TRANSMIT M F R ALL ROUND TRAINING FREQUENY</td>\n",
       "      <td>ROGER WEAPS PWO TRANSMIT M F R ALL ROUND TRAINING FREQUENCY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALPFA FOUR QUEBEC SHOT OUT</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC SHOT OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAL STATION PWO SURFACE INVESTIGATE U B [UNK] ZERO THREE ZERO BEARING [UNK] EIGHT DECIMAL CORRCTION BEARING ZERO ZERO ZERO RANGE EIGHT DECIMAL NINE</td>\n",
       "      <td>ALL STATION PWO SURFACE INVESTIGATE U B [UNK] ZERO THREE ZERO BEARING [UNK] EIGHT DECIMAL CORRECTION BEARING ZERO ZERO ZERO RANGE EIGHT DECIMAL NINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THREE MILES</td>\n",
       "      <td>THREE MILES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SURFACE THREAT WARNING YELLOW TPS OOW PWO INFORM ONCE [UNK] READY TO HAND OVER OPS CONTROL THIS IS PWO STERING ZERO ZERO ONE SPEED TEN I HAVE OPS CONTROL OOW PWO</td>\n",
       "      <td>SURFACE THREAT WARNING YELLOW TPS OOW PWO INFORM ONCE [UNK] READY TO HAND OVER OPS CONTROL THIS IS PWO STEERING ZERO ZERO ONE SPEED TEN I HAVE OPS CONTROL OOW PWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INTEND TO CONDUCT P A C ONE NAUTICAL MILES SOUTH OF STARTEX INTEND [UNK] FIRST ROUND OUT AT ONE ZERO THREE FIVE</td>\n",
       "      <td>INTEND TO CONDUCT P A C ONE NAUTICAL MILES SOUTH OF STARTEX INTEND FIRST ROUND OUT AT ONE ZERO THREE FIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AL FOXTROT ONE DELTA THIS IS ALPHA FOUR QUEBEC</td>\n",
       "      <td>ALFA FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WEAPS PWO UPGRADE A GUN READINESS  STATE TWO OOOW [UNK] PWO INCREASE TWO SEVEN  EW PWO CHECK WATS THE EW LION ZERO ZERO ZERO TWO</td>\n",
       "      <td>WEAPS PWO UPGRADE A GUN READINESS TO STATE TWO OOW [UNK] PWO INCREASE TWO SEVEN [UNK] EW PWO WHAT'S THE EW LINE [UNK] ZERO ZERO TWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PERMISSION TO UPGRADE A GUN READINES STATE TWO WEAPS PWO UPGRADE A GUN READINES  STATE TWO ALL IN PWO ACCESS  WILL BE RACHING [UNK]</td>\n",
       "      <td>PWO PERMISSION TO UPGRADE A GUN READINESS TO STATE TWO WEAPS PWO UPGRADE A GUN READINESS TO STATE TWO ALL IN PWO ASSESS WE WILL BE REACHING [UNK]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e43e9-493b-4e16-889d-4d375959409c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d3bd99e-4f3c-47d4-b5c5-74adc35ac1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PWO A PROVE TRANSMIT FCR</td>\n",
       "      <td>PWO APPROVE TRANSMIT FCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENGAGE [UNK] P A C TRACK AT [UNK] THREE MILES OW PWO PWO ROGER A GUN CEASE FIRING WEAPS PWO ROGER PWOW HAUL DOWN FLAG BRAVO</td>\n",
       "      <td>ENGAGE [UNK] P A C TRACK AT [UNK] THREE MILES OOW PWO PWO ROGER A GUN CEASE FIRE WEAPS PWO ROGER PWO OOW HAUL DOWN FLAG BRAVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMMAND THIS IS PWO K RE FOR THIS INARIAL KHREE F FFSS TWO HAS READY BEN INSTRUCTED TO POSITAKE UP POSITION IN EMPIRING POSITION THREE MILES [UNK] EAST OF J R INTHIS CASE [UNK] OWN FORCE ALFA FOUR QUEBEC WILL DIRECT  FOXTROT ONE DELTA TO CONDUCT DIVERSION FOR QUEBEC FOXTROT BEARING [UNK] ZERO TWO FIVE RANGE SEVEN MILES</td>\n",
       "      <td>COMMAND THIS PWO K [UNK] FOR THIS SCENARIO THREE FFS TWO HAVE ALREADY BEEN INSTRUCTED TO [UNK] TAKE UP POSITION EMPIRE POSITION THREE MILE [UNK] EAST OF JR THIS CASE [UNK] OWN FORCE ALPHA FOUR QBAT WILL DIRECT [UNK] FOXTORT ONE DETLA TO CONDUCT DIVSERION FOR QBAT FOXTROT BEARING [UNK] ZERO TWO FIVE RANGE SEVEN MILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THREE ZERO ZERO TRANSMIT F C R OW COME RIGHT THREE TWO ZERO</td>\n",
       "      <td>THREE ZERO ZERO TRANSMIT F C R OOW COME RIGHT THREE TWO ZERO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGUN COVER ZERO ZERO SEVEN THREE FCS CORRECTION A GUN COVER ZERO ZERO NINE FOUR</td>\n",
       "      <td>AGUN COVER ZERO ZERO SEVEN THREE CORRECTION AGUN COVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALL GUNERY POSITION SGUNERY BROADCAST POLICY SURFACE AND AIR AGUN PROVIDE MODE ALPHA LOADING BRAVO GUN PROVIDE ONE BOX H T G  M PROVIDE ONE BOX TWO FIVE ZERO ROUNDS LOOKOUT ALL ROUND LOKOUT ROUTINE STANDARD FOUR FIVE ALLGUNERY POSITIONS STAND TO FCS</td>\n",
       "      <td>ALL GUNNERY POSITIONS GUNNERY BROADCAST POLICY SURFACE AND AIR AGUN PROVIDE MODE ALPHA LOADING BRAVO GUN PROVIDE ONE BOX H E I T G P M G PROVIDE ONE BOX TWO FIVE ZERO ROUNDS LOOKOUT ALL ROUND LOOKOUT ROUTINE STANDARD FOUR FIVE ALL GUNNERY POSITIONS STAND TO FCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC A GUN ENCOUNTER STOP PAGE A TC THREE ORS INTEND TO RETURN TO BASE FOR A GUN RECTIFICATION WILL BE UNLOLDING REMAINING A GUN ROUNDS AND PROCEEDING BACK FOR PROCUEDIG BAC TO BASE FOR A GUN RECTIFACATION FOXTROT ONE DELTA OVER</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR Q QUEBEC A GUN ENCOUNTER STOPPAGE E T C THREE HOURS INTEND TO RETURN TO BASE FOR A GUN RECTIFICATION WILL BE UNLOADING REMAINING A GUN ROUNDS AND PROCEEDING BACK FOR PROCEEDING BACK TO BASE FOR A GUN RECTIFICATION FOXTROT ONE DELTA OVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FOXTROT ONE DELTA THIS IS LFA FOUR QUEBEC [UNK] COMPLETED P A C FIRING [UNK] PROCEEDING FOR J R FIRING STAND BY FOR FURTHER SERP FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC FOUR YOU J HEAD [UNK]SOUTH TO ELOKATETHE POSITION UNIFORM ALFA TWO THREE THREE FOXTRT OE E</td>\n",
       "      <td>FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC [UNK] COMPLETED P A C FIRING [UNK] PROCEEDING FOR J R FIRING STAND BY FOR FURTHER SITREP FOXTROT ONE DELTA THIS IS ALFA FOUR QUEBEC FOR YOU TO [UNK] HEAD [UNK] SOUTH TO ALLOCATED POSITION UNIFORM ALFA TWO THREE THREE FOXTROT ONE DELTA OVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PWO ROGER OOW PWO PUT FLAG BRAVO AT DIP COM PWO CHECK [UNK] N O C INFORM  AND REQUEST ED SIWER AND INDEL ROGER [UNK] COL N O SCAY AGAIN MINUTE ONE ZERO SHIP WIL COMMENCES [UNK] P A C FIRING MINUTE ONE ZERO ONE FIVE</td>\n",
       "      <td>PWO ROGER OOW PWO PUT FLAG BRAVO AT DIP COMMS PWO CHECK [UNK] N O C INFORM [UNK] OF AND REQUESTED SITREP AND INTEL ROGER [UNK] CALL N O C AGAIN MINUTES ONE ZERO SHIP WILL COMMENCES [UNK] P A C FIRING MINUTES ONE ZERO ONE FIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AL STATION TIME MINUTES TWO ZERO COMMENCES COMMS CHECK OVER THIS IS PWO LOUD AND CLEAR OVER THIS IS PWO ROGER ALL STATION THIS IS PWO MISSION BRIEF IS TO CONDUCT P A C FIRING BEFORE J R S S FIRING IN E OF  TOP GUN ROUDS AFTER ONCE AFTER WE COMPLETED J R S S FOXTROT ONE DELTA WILL CONDUCT [UNK] J R S S FIRING</td>\n",
       "      <td>ALL STATION TIME MINUTUE TWO ZERO COMMENCES COMMS CHECK OVER THIS IS PWO LOUD AND CLEAR OVER THIS IS PWO ROGER ALL STATION THIS IS PWO MISSION BRIEF IS TO CONDUCT P A C FIRING BEFORE J R S S FIRING IN VIEW OF TOP GUN RULES AFTER ONCE AFTER WE COMPLETED J R S S FOXTROT ONE DELTA WILL CONDUCT HER J R S S FIRING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbaa772-71ef-4e1c-becd-c6477a89aac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07e3feb2-7471-429e-8de0-aba3cced0153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F O X [PAD] T T R O O O T | | O O N N E | | [PAD] D E E E L L [PAD] [PAD] T T A A [PAD] [PAD] [PAD] [PAD] | | [PAD] T H H I S S | | [PAD] [PAD] [PAD] I I I S | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] A L L P [PAD] [PAD] F A A [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] F O O U U R R [PAD] [PAD] | | [PAD] [PAD] [PAD] Q U U E E [PAD] [PAD] [PAD] B [PAD] E E C [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] S S H O O O T T | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] M I I N N U U T E E E | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O N N E | | T H H R E E [PAD] [PAD] E E [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O N E | | [PAD] [PAD] F F [PAD] I I V E E E | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O O | | | [PAD] F F | | [PAD] [PAD] [PAD] [PAD] R [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F [PAD] I V E E | | D E C C [PAD] I I I M M A L L [PAD] | | [PAD] [PAD] F O O U U R R [PAD] [PAD] | | [PAD] [PAD] [PAD] [PAD] O O U U T T | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F O O X [PAD] T R R O O T | | [PAD] O N N E E | | [PAD] [PAD] D E E L L [PAD] [PAD] [PAD] T A A [PAD] [PAD] [PAD] [PAD] | | [PAD] [PAD] T H I I S | | [PAD] I I S | | [PAD] [PAD] [PAD] [PAD] [PAD] A A L L P P [PAD] [PAD] [PAD] F A A [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] [PAD] F F O U U U R R R [PAD] [PAD] | | [PAD] [PAD] Q U U E E [PAD] [PAD] B [PAD] E E C | | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] A L L [PAD] F A A [PAD] [PAD] | | [PAD] F O O U U R R [PAD] | [PAD] [PAD] Q U U E [PAD] [PAD] B [PAD] E C C | | | E E N N [PAD] [PAD] [PAD] [PAD] C O O U U N N [PAD] [PAD] [PAD] T E E R [PAD] [PAD] [PAD] | | [PAD] M M I S S [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] F F I I I R R R E E [PAD] [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] S A A F F E E T Y Y [PAD] [PAD] | [PAD] B E A A R R R I I N N G G | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Z Z E E R O O [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Z E E R R O O [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] Z Z E E R R O [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] I N N [PAD] [PAD] T [PAD] E E N N D D | | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] T T [PAD] O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] C O O N N [PAD] [PAD] D [PAD] U C T | | [PAD] [PAD] [PAD] [PAD] [PAD] M M I S S [PAD] [PAD] [PAD] [PAD] [PAD] F [PAD] I I R R R E E [PAD] [PAD] [PAD] | | [PAD] [PAD] [PAD] Z Z E E [PAD] [PAD] O O O [PAD] [PAD] [PAD] [PAD] | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] B E E A [PAD] R I I N N G | [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] Z Z E E R O O [PAD] [PAD] | | [PAD] [PAD] Z E E R O O [PAD] [PAD] | [PAD] Z Z E R R O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(dataset[\"dev\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70d4be-9d45-4c3a-bddc-bc4f0de924e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf4e9a56-0730-406b-bf1d-d3f1ec5519b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] O O O O [PAD] O [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] W W [PAD] | | | I I N N [PAD] C C R R E A A S S E E | | [PAD] [PAD] [PAD] O O N N E | | [PAD] T T H R R E E [PAD] [PAD] E E [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(dataset[\"test\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# convert ids to tokens\n",
    "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52212dc8-3027-4665-8cbc-dad1a9b1c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
