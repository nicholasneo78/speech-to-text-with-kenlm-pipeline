# configuration for the librispeech dataset with clearml

# task initialisation
project_name: 'wav2vec2_kenlm_pipeline'
task_name: 'finetuning'
dataset_name: 'librispeech_v2_finetuning'
output_url: 's3://experiment-logging' #/storage'
dataset_project: 'datasets/librispeech'

# arguments corresponding to the data_preprocessing.py
dataset_task_id: '008d58167e9248e48f9433e43fc30671'
dataset_task_id_w2v2_base: 'bf5a0962b65946ec92b67ddc690532b1'

train_pkl: 'pkl/librispeech_train.pkl'
dev_pkl: 'pkl/librispeech_dev.pkl'
test_pkl: 'pkl/librispeech_test.pkl'
processor_path: './processor/'
checkpoint_path: './ckpt/'
pretrained_model_path: 'wav2vec2_base_model/'
saved_model_path: './saved_model/'
max_sample_length: 450000
batch_size: 8
epochs: 10
lr: 1e-4
weight_decay: 0.005
warmup_steps: 1000
finetune_from_scratch: True 

# queue name
queue: 'compute'