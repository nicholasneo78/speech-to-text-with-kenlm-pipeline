# configuration for the librispeech dataset with clearml

# task initialisation
project_name: 'wavlm_kenlm_pipeline'
task_name: 'evaluation_with_lm'
output_url: 's3://experiment-logging'

# arguments corresponding to the finetuning.py
dataset_pkl_task_id: 'fdb1e1471ebb4b8dbf4f599080401819' # pkl files
dataset_finetuned_task_id: '435eb59d57fb4b16ad864183fd23e3e6' # the finetuned model
lm_id: '3366eaba06054260a40b02c6f9277dce' # kenlm arpa file

test_pkl: 'pkl/librispeech_test.pkl'
finetuned_model_path: 'saved_model/'
input_processor_path: 'processor/'
lm_path: 'lm/5_gram_librispeech.arpa'

alpha: 0.6
beta: 1.0
architecture: 'wavlm'

queue: 'compute2'